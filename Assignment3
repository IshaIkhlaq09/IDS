# 11/26/23
# CSC461 – Assignment3 – Machine Learning
# Isha Ikhlaq
# Sp20-BSE-009
# Q#2 Apply Logistic Regression, Support Vector Machines, and Multilayer Perceptron classification
algorithms (using Python) on the gender prediction dataset with 2/3 train and 1/3 test split ratio and answer
the following questions.


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Upload the "gender-prediction.csv" file in Google Colab
from google.colab import files
uploaded = files.upload()

# Read the CSV file into a DataFrame
import io
df = pd.read_csv(io.StringIO(uploaded['gender-prediction.csv'].decode('utf-8')))

# Display the first few rows of the dataset
print(df.head())

# Separate features (X) and target variable (y)
X = df.drop('gender', axis=1)
y = df['gender']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)

# Make predictions
y_pred = logreg.predict(X_test_scaled)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Logistic Regression Accuracy:", accuracy)
# Task : Name 2 attributes that you believe are the most "powerful" in the prediction task. Explain why?
# let's assume that 'Height' and 'Weight' are considered the most powerful attributes.
# These attributes could be influential in predicting gender as they often show differences between males and females.

# Task : Try to exclude these 2 attributes(s) from the dataset. Rerun the experiment with 80/20 train/test split.
# Check for any change in the results and explain.

# Your code for excluding attributes and rerunning the experiment
X_excluded = X.drop(['Height', 'Weight'], axis=1)

X_train_excluded, X_test_excluded, y_train_excluded, y_test_excluded = train_test_split(X_excluded, y, test_size=0.2, random_state=42)

# Apply Logistic Regression without the excluded attributes
lr_model_excluded = LogisticRegression()
lr_model_excluded.fit(X_train_excluded, y_train_excluded)
lr_predictions_excluded = lr_model_excluded.predict(X_test_excluded)
lr_accuracy_excluded = accuracy_score(y_test_excluded, lr_predictions_excluded)

# Apply Support Vector Machines without the excluded attributes
svm_model_excluded = SVC()
svm_model_excluded.fit(X_train_excluded, y_train_excluded)
svm_predictions_excluded = svm_model_excluded.predict(X_test_excluded)
svm_accuracy_excluded = accuracy_score(y_test_excluded, svm_predictions_excluded)

# Apply Multilayer Perceptron without the excluded attributes
mlp_model_excluded = MLPClassifier()
mlp_model_excluded.fit(X_train_excluded, y_train_excluded)
mlp_predictions_excluded = mlp_model_excluded.predict(X_test_excluded)
mlp_accuracy_excluded = accuracy_score(y_test_excluded, mlp_predictions_excluded)

# Print results for the excluded attributes
print("\nTask 6: Results after excluding 'Height' and 'Weight':")
print("Logistic Regression Accuracy:", lr_accuracy_excluded)
print("Support Vector Machines Accuracy:", svm_accuracy_excluded)
print("Multilayer Perceptron Accuracy:", mlp_accuracy_excluded)

#  Print the number of instances incorrectly classified for each model without the excluded attributes
incorrect_lr_excluded = (y_test_excluded != lr_predictions_excluded).sum()
incorrect_svm_excluded = (y_test_excluded != svm_predictions_excluded).sum()
incorrect_mlp_excluded = (y_test_excluded != mlp_predictions_excluded).sum()

print("\nTask 6: Number of instances incorrectly classified without 'Height' and 'Weight':")
print("Logistic Regression:", incorrect_lr_excluded)
print("Support Vector Machines:", incorrect_svm_excluded)
print("Multilayer Perceptron:", incorrect_mlp_excluded)


# 11/26/23
# CSC461 – Assignment3 – Machine Learning
# Isha Ikhlaq
# Sp20-BSE-009
# Q#3 Apply Random Forest classification algorithm (using Python) on the gender prediction dataset with Monte
Carlo cross-validation and Leave P-Out cross-validation. Report F1 scores for both cross-validation strategies.

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, KFold, LeavePOut
from sklearn.metrics import f1_score

# Load  dataset (replace 'your_dataset.csv' with the actual dataset file)
# Assume the dataset has columns representing features and a 'gender' column as the target variable
data = pd.read_csv('your_dataset.csv')

# Assuming 'X' contains the features and 'y' contains the target variable
X = data.drop('gender', axis=1)
y = data['gender']

# Random Forest Classifier initialization with chosen parameter values
random_forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)

# Define the number of iterations for Monte Carlo cross-validation
num_iterations = 5  # You can adjust this value

# Monte Carlo cross-validation
mc_f1_scores = cross_val_score(random_forest, X, y, cv=num_iterations, scoring='f1_macro')

print(f"Monte Carlo Cross-Validation F1 Scores: {mc_f1_scores}")
print(f"Average F1 Score for Monte Carlo Cross-Validation: {mc_f1_scores.mean()}")

# Define the number of samples to leave out for Leave P-Out cross-validation
leave_p_out = 2  # You can adjust this value

# Leave P-Out cross-validation
lp_f1_scores = []
leave_p_out_cv = LeavePOut(leave_p_out)
for train_index, test_index in leave_p_out_cv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    random_forest.fit(X_train, y_train)
    y_pred = random_forest.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='macro')
    lp_f1_scores.append(f1)

print(f"Leave P-Out Cross-Validation F1 Scores: {lp_f1_scores}")
print(f"Average F1 Score for Leave P-Out Cross-Validation: {sum(lp_f1_scores) / len(lp_f1_scores)}")


# 11/26/23
# CSC461 – Assignment3 – Machine Learning
# Isha Ikhlaq
# Sp20-BSE-009
# Q#4 Add 10 sample instances into the dataset (you can ask your friends/relatives/sibling for the data). Run the
ML experiment (using Python) by training the model using Gaussian Naïve Bayes classification algorithm
and all the instances from the gender prediction dataset. Evaluate the trained model using the newly added 10
test instances. Report accuracy, precision, and recall scores.

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score


# Concatenate the existing dataset and the 10 new test instances
merged_data = pd.concat([data, new_test_data], ignore_index=True)

# Assuming 'X' contains the features and 'y' contains the target variable for the merged dataset
X_merged = merged_data.drop('gender', axis=1)
y_merged = merged_data['gender']

# Separate the merged dataset into training and testing sets
# Use the first len(data) instances for training and the remaining 10 instances for testing
X_train = X_merged.iloc[:len(data)]
X_test = X_merged.iloc[len(data):]
y_train = y_merged.iloc[:len(data)]
y_test = y_merged.iloc[len(data):]

# Train Gaussian Naïve Bayes classifier using all instances from the gender prediction dataset
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# Evaluate the trained model using the 10 new test instances
y_pred = gnb.predict(X_test)

# Calculate accuracy, precision, and recall scores
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')

# Report the evaluation metrics
print(f"Accuracy Score: {accuracy:.4f}")
print(f"Precision Score: {precision:.4f}")
print(f"Recall Score: {recall:.4f}")

